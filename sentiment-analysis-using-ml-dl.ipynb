{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/abaliyan/sentiment-analysis-using-ml-dl?scriptVersionId=142755006\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# DataCard\n\nIMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n\nThis is a dataset for binary sentiment classification. There are around 50,000 highly polar movie reviews. This can be used to analyse/predict positive and negative reviews using either classifiaction or deep learning algorithm.\n\n# About Dataset\n- Two column: \"review\" and \"sentiment\". \n- review column include the text comment from user and Sentiment column include if this comment is positive or negative ","metadata":{"id":"-76hGsB4-2Mw"}},{"cell_type":"markdown","source":"# Code with python","metadata":{}},{"cell_type":"code","source":"# importing libraries \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud\nimport re\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom  sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,classification_report\n","metadata":{"id":"BkrgzFdj-qer","execution":{"iopub.status.busy":"2023-08-30T05:56:39.589445Z","iopub.execute_input":"2023-08-30T05:56:39.589801Z","iopub.status.idle":"2023-08-30T05:56:39.596819Z","shell.execute_reply.started":"2023-08-30T05:56:39.589774Z","shell.execute_reply":"2023-08-30T05:56:39.595712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import dataset using read_csv method from pandas \ndata = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","metadata":{"id":"qoRrIAkj_Bq1","execution":{"iopub.status.busy":"2023-08-30T05:56:39.604416Z","iopub.execute_input":"2023-08-30T05:56:39.604843Z","iopub.status.idle":"2023-08-30T05:56:40.155972Z","shell.execute_reply.started":"2023-08-30T05:56:39.604787Z","shell.execute_reply":"2023-08-30T05:56:40.154645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check dataset\ndata.head()","metadata":{"id":"afBW5LdU_QkM","outputId":"0c554218-2489-480f-f4cf-e59d42ff6156","execution":{"iopub.status.busy":"2023-08-30T05:56:40.157961Z","iopub.execute_input":"2023-08-30T05:56:40.158289Z","iopub.status.idle":"2023-08-30T05:56:40.166922Z","shell.execute_reply.started":"2023-08-30T05:56:40.158261Z","shell.execute_reply":"2023-08-30T05:56:40.165963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.tail()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T05:56:40.167798Z","iopub.execute_input":"2023-08-30T05:56:40.168063Z","iopub.status.idle":"2023-08-30T05:56:40.183494Z","shell.execute_reply.started":"2023-08-30T05:56:40.16804Z","shell.execute_reply":"2023-08-30T05:56:40.182211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# analysing count of positive and negative sentiments\nprint(pd.DataFrame(data['sentiment'].value_counts()))\nplt.pie(x=data['sentiment'].value_counts(), labels=data['sentiment'].value_counts().keys(), autopct='%1.1f%%')\nplt.title(\"Share of positive and negative reviews\")\nplt.show()","metadata":{"id":"Yi3HfTBL_9UV","outputId":"78997bcc-7649-420d-ccfa-dc8b81d85f22","execution":{"iopub.status.busy":"2023-08-30T05:56:40.186015Z","iopub.execute_input":"2023-08-30T05:56:40.186336Z","iopub.status.idle":"2023-08-30T05:56:40.287921Z","shell.execute_reply.started":"2023-08-30T05:56:40.186308Z","shell.execute_reply":"2023-08-30T05:56:40.286969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- dataset is balanced with equal number of positive and negative observations ","metadata":{}},{"cell_type":"code","source":"# downloading \"stopwords\" and \"wordnet\" from \"Natural Language Toolkit\" to play with text data \nnltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"id":"2Et2Xe2bABk-","outputId":"78cf0f54-03ca-4a50-f179-c747fabec164","execution":{"iopub.status.busy":"2023-08-30T05:56:40.289251Z","iopub.execute_input":"2023-08-30T05:56:40.289531Z","iopub.status.idle":"2023-08-30T05:56:40.41856Z","shell.execute_reply.started":"2023-08-30T05:56:40.289506Z","shell.execute_reply":"2023-08-30T05:56:40.417641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokenization, stopword removal, text lowercase, and stemming\ndef preprocess_text(text):\n    \"\"\"function that take text input and apply below operation to clean the text.\n    1. removing special characters\n    2. create token from the text input and get lowercase\n    3. remove stopwords as per NLTK stopwords library\n    4. using PorterStemmer for stemming the token\n    5. combine all tokens back to text \n    6. return the processed text\"\"\"\n    \n    text = re.sub(r'<[^>]+>', ' ', text)\n    tokens = word_tokenize(text)\n    stop_words = set(stopwords.words('english'))\n    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n    stemmed_tokens = [PorterStemmer().stem(token) for token in filtered_tokens]\n    preprocessed_text = ' '.join(stemmed_tokens)\n    return preprocessed_text.lower()\n\ndata['review'] = data['review'].apply(preprocess_text)","metadata":{"id":"O8C5kG89_1fv","execution":{"iopub.status.busy":"2023-08-30T05:56:40.419825Z","iopub.execute_input":"2023-08-30T05:56:40.420827Z","iopub.status.idle":"2023-08-30T06:01:33.275115Z","shell.execute_reply.started":"2023-08-30T05:56:40.420789Z","shell.execute_reply":"2023-08-30T06:01:33.273989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Map sentiment labels to binary values (0 for negative, 1 for positive)\ndata['sentiment'] = data['sentiment'].map({'negative': 0, 'positive': 1})\ndata.head()","metadata":{"id":"WKZr5ObnAjtj","outputId":"430bf770-447a-4200-f3af-bf0d987c1995","execution":{"iopub.status.busy":"2023-08-30T06:01:33.276941Z","iopub.execute_input":"2023-08-30T06:01:33.277372Z","iopub.status.idle":"2023-08-30T06:01:33.293063Z","shell.execute_reply.started":"2023-08-30T06:01:33.277336Z","shell.execute_reply":"2023-08-30T06:01:33.291792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data for test and training dataset \nX_train,X_test,y_train,y_test=train_test_split(data.review,data.sentiment,test_size=0.5, random_state=42)","metadata":{"id":"dT8zd-EnISxB","execution":{"iopub.status.busy":"2023-08-30T06:01:33.2943Z","iopub.execute_input":"2023-08-30T06:01:33.294616Z","iopub.status.idle":"2023-08-30T06:01:33.315891Z","shell.execute_reply.started":"2023-08-30T06:01:33.294591Z","shell.execute_reply":"2023-08-30T06:01:33.314077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ML model employed\n- LogisticRegression\n- RandomForestClassifier\n- KNeighbourClassifier\n- MultinomialNB","metadata":{"id":"EyBoANcs_TPT"}},{"cell_type":"code","source":"result_summary = pd.DataFrame(columns=['Model','Precision','Recall'])\nresult_summary","metadata":{"execution":{"iopub.status.busy":"2023-08-30T06:01:33.317475Z","iopub.execute_input":"2023-08-30T06:01:33.317822Z","iopub.status.idle":"2023-08-30T06:01:33.328162Z","shell.execute_reply.started":"2023-08-30T06:01:33.317794Z","shell.execute_reply":"2023-08-30T06:01:33.326627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef score_model(model,test_predictions, result_summary):\n    \"\"\"Function to get the summary of the predictions obtained.\n    prints the accuracy, precision score, recall score, confusion matrix, and classification report.\"\"\"\n    \n    print(\"\\nSummary Report\\n\")\n    accuracy = accuracy_score(y_test, test_predictions)\n    print(\"Accuracy:\", accuracy)\n\n    # Calculate precision and recall\n    precision = precision_score(y_test, test_predictions)\n    recall = recall_score(y_test, test_predictions)\n    print(\"Precision:\", precision)\n    print(\"Recall:\", recall)\n\n    # confusion matrix\n    conf_matrix = confusion_matrix(y_test, test_predictions)\n    print(\"\\nConfusion Matrix:\")\n    print(conf_matrix)\n\n    # Generate classification report\n    class_names = ['negative', 'positive']  # Replace with your class labels\n    classification_rep = classification_report(y_test, test_predictions, target_names=class_names)\n    print(\"\\nClassification Report:\")\n    print(classification_rep)\n    print()\n    \n    # adding result to summary report\n    temp_row = pd.DataFrame([{'Model':model,'Precision':precision,'Recall':recall}])\n    result_summary=pd.concat([result_summary, temp_row], ignore_index=True) \n    return(result_summary)","metadata":{"id":"LA-frmmvssW3","execution":{"iopub.status.busy":"2023-08-30T06:01:33.332348Z","iopub.execute_input":"2023-08-30T06:01:33.332738Z","iopub.status.idle":"2023-08-30T06:01:33.342509Z","shell.execute_reply.started":"2023-08-30T06:01:33.332701Z","shell.execute_reply":"2023-08-30T06:01:33.340995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create pipeline for CountVectorizer and LogisticRegression\n\nlr=Pipeline([('vectorizer', CountVectorizer()),('classifier',LogisticRegression())])\n\nlr.fit(X_train,y_train)\n\ny_pred=lr.predict(X_test)\n\nresult_summary = score_model('LogisticRegression',y_pred,result_summary)\nprint(result_summary)","metadata":{"id":"gGdoS9zxAq5c","outputId":"3121c819-a3a7-4655-e075-0277169965d6","execution":{"iopub.status.busy":"2023-08-30T06:01:33.345674Z","iopub.execute_input":"2023-08-30T06:01:33.345981Z","iopub.status.idle":"2023-08-30T06:01:41.859649Z","shell.execute_reply.started":"2023-08-30T06:01:33.345953Z","shell.execute_reply":"2023-08-30T06:01:41.858718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# applying RandomForestClassifier\n\nrfc = Pipeline([('vectorizer', CountVectorizer()),                                                    #initializing the vectorizer\n    ('random_forest', (RandomForestClassifier(n_estimators=50, criterion='entropy')))      #using the RandomForest classifier\n])\n\nrfc.fit(X_train, y_train)\n\ny_pred = rfc.predict(X_test)\n\nresult_summary = score_model('RandomForestClassifier',y_pred, result_summary)\nprint(result_summary)","metadata":{"id":"r9vyJd6WTogq","outputId":"ecc110ad-550c-44b3-bf68-fa438ae7c6cf","execution":{"iopub.status.busy":"2023-08-30T06:01:41.860864Z","iopub.execute_input":"2023-08-30T06:01:41.861416Z","iopub.status.idle":"2023-08-30T06:02:17.28755Z","shell.execute_reply.started":"2023-08-30T06:01:41.861382Z","shell.execute_reply":"2023-08-30T06:02:17.286634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNeighboursClassifier","metadata":{"id":"tY5sKrVru97K"}},{"cell_type":"code","source":"# applying KNeighborsClassifier\n\nknc = Pipeline([\n     ('vectorizer', CountVectorizer()),\n      ('KNN', (KNeighborsClassifier(n_neighbors=10, metric = 'euclidean')))   #using the KNN classifier with 10 neighbors\n])\n\nknc.fit(X_train, y_train)\n\ny_pred = knc.predict(X_test)\n\nresult_summary = score_model('KNeighborsClassifier',y_pred,result_summary)\nresult_summary","metadata":{"id":"jUjx9Xh7VIBO","outputId":"3b939c1f-43b2-467f-a30b-df4d060506d7","execution":{"iopub.status.busy":"2023-08-30T06:02:17.288778Z","iopub.execute_input":"2023-08-30T06:02:17.289109Z","iopub.status.idle":"2023-08-30T06:03:15.7282Z","shell.execute_reply.started":"2023-08-30T06:02:17.289078Z","shell.execute_reply":"2023-08-30T06:03:15.727041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MultinomialNB","metadata":{"id":"N2h_wq21xIPl"}},{"cell_type":"code","source":"# applying MultinomialNB\n\nmnb = Pipeline([\n    ('vectorizer', CountVectorizer()),\n    ('nb', MultinomialNB())\n])\n\nmnb.fit(X_train, y_train)\n\ny_pred = mnb.predict(X_test)\n\nresult_summary = score_model('MultinomialNB',y_pred,result_summary)\nresult_summary","metadata":{"id":"1krDUDB7xHX0","outputId":"6a15fa56-b348-4df1-a7bd-3420116ca44e","execution":{"iopub.status.busy":"2023-08-30T06:03:15.729426Z","iopub.execute_input":"2023-08-30T06:03:15.729737Z","iopub.status.idle":"2023-08-30T06:03:20.160112Z","shell.execute_reply.started":"2023-08-30T06:03:15.729712Z","shell.execute_reply":"2023-08-30T06:03:20.158654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ConvolutionNeuralNetwork","metadata":{"id":"xko26O3Y5tko"}},{"cell_type":"code","source":"# importing function and model for CNN\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Flatten,Conv1D, GlobalMaxPooling1D, Dense, LSTM\nfrom tensorflow.keras.activations import relu, sigmoid\n","metadata":{"id":"uA7VkEOB58-z","execution":{"iopub.status.busy":"2023-08-30T06:03:20.161764Z","iopub.execute_input":"2023-08-30T06:03:20.162114Z","iopub.status.idle":"2023-08-30T06:03:20.168119Z","shell.execute_reply.started":"2023-08-30T06:03:20.162084Z","shell.execute_reply":"2023-08-30T06:03:20.167005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessing text\n\nlm = WordNetLemmatizer()\ndef text_processing(df_col):\n    corpus = []\n    for item in df_col:\n        new_item = re.sub('[^a-zA-Z]',' ',str(item))\n        new_item = new_item.lower()\n        new_item = new_item.split()\n        new_item = [lm.lemmatize(word) for word in new_item if word not in set(stopwords.words('english'))]\n        corpus.append(' '.join(str(x) for x in new_item))\n    return corpus\n\n# data=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/project work/IMDB movies review sentiment analysis/IMDB Dataset.csv\")\ndf=data.copy()\n# df['review'] = text_processing(df['review'])","metadata":{"id":"MWFijCnM84mW","execution":{"iopub.status.busy":"2023-08-30T06:03:20.169637Z","iopub.execute_input":"2023-08-30T06:03:20.16995Z","iopub.status.idle":"2023-08-30T06:03:20.185201Z","shell.execute_reply.started":"2023-08-30T06:03:20.169924Z","shell.execute_reply":"2023-08-30T06:03:20.184153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into train and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.5, random_state=42)","metadata":{"id":"0DLF36-G87V3","execution":{"iopub.status.busy":"2023-08-30T06:03:20.186717Z","iopub.execute_input":"2023-08-30T06:03:20.187101Z","iopub.status.idle":"2023-08-30T06:03:20.20898Z","shell.execute_reply.started":"2023-08-30T06:03:20.187072Z","shell.execute_reply":"2023-08-30T06:03:20.207784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenization and padding\nmax_words = 1000  # Maximum number of words in your vocabulary\nmax_seq_length = 100  # Maximum length of sequences (words in a review)\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(train_df['review'])\n\nX_train = tokenizer.texts_to_sequences(train_df['review'])\nX_test = tokenizer.texts_to_sequences(test_df['review'])\n\nX_train = pad_sequences(X_train, maxlen=max_seq_length, padding = 'post')\nX_test = pad_sequences(X_test, maxlen=max_seq_length, padding='post')\n\ny_train = np.array(train_df['sentiment'])\ny_test = np.array(test_df['sentiment'])","metadata":{"id":"f9JPbULgyjPW","execution":{"iopub.status.busy":"2023-08-30T06:03:20.209873Z","iopub.execute_input":"2023-08-30T06:03:20.210133Z","iopub.status.idle":"2023-08-30T06:03:27.386381Z","shell.execute_reply.started":"2023-08-30T06:03:20.21011Z","shell.execute_reply":"2023-08-30T06:03:27.384946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the CNN model\nembedding_dim = 100\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_seq_length))\nmodel.add(Conv1D(filters=128, kernel_size=5, activation=relu))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(64, activation=relu))\nmodel.add(Dense(1, activation=sigmoid))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\n# Train the model\nepochs = 10\nbatch_size = 32\n\nhistory = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on the test set\ntest_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)","metadata":{"id":"WDJHScX952N9","outputId":"4fa2d629-0ee6-4288-f485-f81c3afdc6d6","execution":{"iopub.status.busy":"2023-08-30T06:03:27.387955Z","iopub.execute_input":"2023-08-30T06:03:27.388259Z","iopub.status.idle":"2023-08-30T06:05:24.767441Z","shell.execute_reply.started":"2023-08-30T06:03:27.388231Z","shell.execute_reply":"2023-08-30T06:05:24.765329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\n\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_loss, label='Train Loss', marker='o')\nplt.plot(val_loss, label='Validation Loss', marker='o')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid()\nplt.legend()\n\n\nplt.subplot(1, 2, 2)\nplt.plot(train_accuracy, label='Train Accuracy', marker='o')\nplt.plot(val_accuracy, label='Validation Accuracy', marker='o')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.grid()\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\ntest_predictions_probs = model.predict(X_test)\ntest_predictions = (test_predictions_probs > 0.5).astype(int)  # Convert probabilities to binary predictions\n#score_model(test_predictions)\n\nresult_summary = score_model('CNN',test_predictions,result_summary)","metadata":{"id":"SbIzBzGV7RaY","outputId":"193aa53e-b317-49bc-e376-93eff1d4d6cc","execution":{"iopub.status.busy":"2023-08-30T06:05:24.76936Z","iopub.execute_input":"2023-08-30T06:05:24.769836Z","iopub.status.idle":"2023-08-30T06:05:30.015589Z","shell.execute_reply.started":"2023-08-30T06:05:24.769803Z","shell.execute_reply":"2023-08-30T06:05:30.013884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_summary","metadata":{"execution":{"iopub.status.busy":"2023-08-30T06:05:30.016773Z","iopub.execute_input":"2023-08-30T06:05:30.017075Z","iopub.status.idle":"2023-08-30T06:05:30.029101Z","shell.execute_reply.started":"2023-08-30T06:05:30.01705Z","shell.execute_reply":"2023-08-30T06:05:30.027794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above result summary, we can conclude that the logistic regression is performing the best in classififying the sentiments.","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}